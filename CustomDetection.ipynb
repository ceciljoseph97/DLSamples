{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37dabebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (1.8.1+cu111)\n",
      "Requirement already satisfied: torchvision in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (0.9.1+cu111)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (from torch) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (9.2.0)\n"
     ]
    }
   ],
   "source": [
    "#pytorch main site\n",
    "!pip install torch torchvision torchaudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70db681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.23.4 in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 1)) (1.23.4)\n",
      "Requirement already satisfied: pandas==1.4.1 in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (from pandas==1.4.1->-r requirements.txt (line 2)) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (from pandas==1.4.1->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cecil\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas==1.4.1->-r requirements.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/ultralytics/yolov5\n",
    "!cd yolov5\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ea0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #Load YOLO model and make detections\n",
    "from matplotlib import pyplot as plt # visualizing images\n",
    "import numpy as np #array transformations\n",
    "import cv2 # access and render webcam feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f08e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\cecil/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-12-28 Python-3.9.13 torch-1.8.1+cu111 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5','yolov5s') # pretrained baseline model is downloaded into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d69379a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 720x1280 1 person\n",
      "Speed: 32.0ms pre-process, 148.0ms inference, 10.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "img = 'cecilHappy.jpg'\n",
    "result=model(img)\n",
    "result.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46a1b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#plt.imshow(np.squeeze(result.render()))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67594b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 720, 1280, 3)\n",
      "(720, 1280, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x246a67d6820>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result.show()\n",
    "print(np.array(result.render()).shape)\n",
    "print(np.squeeze(result.render()).shape)\n",
    "plt.imshow(np.squeeze(result.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801ee973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV2 vid Capture\n",
    "def vidCapDefaultMode():\n",
    "    vidCap=cv2.VideoCapture(0)\n",
    "    while vidCap.isOpened():\n",
    "        ret, frame =vidCap.read()\n",
    "        \n",
    "        cv2.imshow('Video Capture',frame)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "    vidCap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "vidCapDefaultMode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "814191e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOLO Pretrained Real time Detection\n",
    "def vidCapYOLODetection():\n",
    "    vidCap=cv2.VideoCapture(0) # cv2.VideoCapture('vidName.mp4') for detection on a video.\n",
    "    while vidCap.isOpened():\n",
    "        ret, frame =vidCap.read()\n",
    "        # detection made from model, Passing the frame into model and render the results of the prediction.\n",
    "        rslt=model(frame)\n",
    "        cv2.imshow('YOLO Default Pretrained Detection',np.squeeze(rslt.render()))\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "    vidCap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "#vidCapYOLODetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794b3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid # unique identifier\n",
    "import os # leveraging file opns\n",
    "import time # Collection param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddd1ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGES_PATH=os.path.join('data','images') # /data/images\n",
    "#labels =[\"Alive\",\"Zombie\"]\n",
    "IMAGES_PATH=os.path.join('smileData','images')\n",
    "labels =[\"Smiling\",\"Not_Smiling\"]\n",
    "num_imgs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59f0306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Images for Smiling\n",
      "Collecting images for Smiling, image number 0\n",
      "Collecting images for Smiling, image number 1\n",
      "Collecting images for Smiling, image number 2\n",
      "Collecting images for Smiling, image number 3\n",
      "Collecting images for Smiling, image number 4\n",
      "Collecting images for Smiling, image number 5\n",
      "Collecting images for Smiling, image number 6\n",
      "Collecting images for Smiling, image number 7\n",
      "Collecting images for Smiling, image number 8\n",
      "Collecting images for Smiling, image number 9\n",
      "Collecting images for Smiling, image number 10\n",
      "Collecting images for Smiling, image number 11\n",
      "Collecting images for Smiling, image number 12\n",
      "Collecting images for Smiling, image number 13\n",
      "Collecting images for Smiling, image number 14\n",
      "Collecting images for Smiling, image number 15\n",
      "Collecting images for Smiling, image number 16\n",
      "Collecting images for Smiling, image number 17\n",
      "Collecting images for Smiling, image number 18\n",
      "Collecting images for Smiling, image number 19\n",
      "Collecting Images for Not_Smiling\n",
      "Collecting images for Not_Smiling, image number 0\n",
      "Collecting images for Not_Smiling, image number 1\n",
      "Collecting images for Not_Smiling, image number 2\n",
      "Collecting images for Not_Smiling, image number 3\n",
      "Collecting images for Not_Smiling, image number 4\n",
      "Collecting images for Not_Smiling, image number 5\n",
      "Collecting images for Not_Smiling, image number 6\n",
      "Collecting images for Not_Smiling, image number 7\n",
      "Collecting images for Not_Smiling, image number 8\n",
      "Collecting images for Not_Smiling, image number 9\n",
      "Collecting images for Not_Smiling, image number 10\n",
      "Collecting images for Not_Smiling, image number 11\n",
      "Collecting images for Not_Smiling, image number 12\n",
      "Collecting images for Not_Smiling, image number 13\n",
      "Collecting images for Not_Smiling, image number 14\n",
      "Collecting images for Not_Smiling, image number 15\n",
      "Collecting images for Not_Smiling, image number 16\n",
      "Collecting images for Not_Smiling, image number 17\n",
      "Collecting images for Not_Smiling, image number 18\n",
      "Collecting images for Not_Smiling, image number 19\n"
     ]
    }
   ],
   "source": [
    "def imageCollection():\n",
    "    vidCap=cv2.VideoCapture(0)\n",
    "    # Loop through labels\n",
    "    for label in labels:\n",
    "        print(\"Collecting Images for {}\".format(label))\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Loop through Image Range\n",
    "        for img_num in range(num_imgs):\n",
    "            print(\"Collecting images for {}, image number {}\".format(label, img_num))\n",
    "            # vid Capture/feed\n",
    "            ret, frame =vidCap.read()\n",
    "            imgName=os.path.join(IMAGES_PATH,label+'.'+str(uuid.uuid1())+'.jpg')\n",
    "            #write out image to file\n",
    "            cv2.imwrite(imgName,frame)\n",
    "            # render to screen\n",
    "            cv2.imshow('Image Collection', frame)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "                break\n",
    "    vidCap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "imageCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c91daf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyqt (from versions: none)\n",
      "ERROR: No matching distribution found for pyqt\n"
     ]
    }
   ],
   "source": [
    "# labelimg\n",
    "#!git clone https://github.com/tzutalin/labelImg\n",
    "#!pip install pyqt lxml --upgrade\n",
    "#!cd labelImg && pyrcc5 -o libs/resources.py resources.qrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "707339a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#incompatible in current laptop, try running in google colab\n",
    "#!cd yolov5 && python train.py --img 320 --batch 16 --epochs 500 --data dataset.yaml --weights yolov5s.pt --device CPU\n",
    "def vidCapDeadDetection():\n",
    "    model = torch.hub.load('ultralytics/yolov5','custom', path=\"yolov5/runs/train/exp/weights/lastN.pt\", force_reload=True)\n",
    "    vidCap=cv2.VideoCapture(0) # cv2.VideoCapture('vidName.mp4') for detection on a video.\n",
    "    while vidCap.isOpened():\n",
    "        ret, frame =vidCap.read()\n",
    "        # detection made from model, Passing the frame into model and render the results of the prediction.\n",
    "        rslt=model(frame)\n",
    "        cv2.imshow('Dead Detection',np.squeeze(rslt.render()))\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "    vidCap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d0e0af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\cecil/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-1-9 Python-3.9.13 torch-1.8.1+cu111 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7055974 parameters, 0 gradients, 15.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "vidCapDeadDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "471cf7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vidCapSmileDetection():\n",
    "    model = torch.hub.load('ultralytics/yolov5','custom', path=\"yolov5/runs/train/exp/weights/lastSmile.pt\", force_reload=True)\n",
    "    vidCap=cv2.VideoCapture(0) # cv2.VideoCapture('vidName.mp4') for detection on a video.\n",
    "    while vidCap.isOpened():\n",
    "        ret, frame =vidCap.read()\n",
    "        # detection made from model, Passing the frame into model and render the results of the prediction.\n",
    "        rslt=model(frame)\n",
    "        cv2.imshow('Smile Detection',np.squeeze(rslt.render()))\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "    vidCap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba795779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\cecil/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-1-9 Python-3.9.13 torch-1.8.1+cu111 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7055974 parameters, 0 gradients, 15.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "vidCapSmileDetection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
